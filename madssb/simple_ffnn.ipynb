{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the input\n",
    "$$\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "0 & 0\\\\\n",
    "1 & 0\\\\\n",
    "0 & 1\\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and outputs\n",
    "$$\n",
    "\\mathbf{y}_\\mathrm{OR} = \\begin{bmatrix}0 \\\\ 1 \\\\ 1 \\\\ 1\\end{bmatrix}, \\mathbf{y}_\\mathrm{XOR} = \\begin{bmatrix}0 \\\\ 1 \\\\ 1 \\\\ 0\\end{bmatrix}, \\mathbf{y}_\\mathrm{AND} = \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple  feed forward neural network (FFNN)\n",
    "We define the activation function with the sigmoid function $\\sigma(x)$ = \n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}.\n",
    "$$\n",
    "We define outputs $a_1^{(1)},a_2^{(1)}$ for the hidden layer with 2 nodes:\n",
    "$$\n",
    "a_1^{(1)} = \\sigma(z_1^{(1)}),\\quad a_2^{(1)} = \\sigma(z_2^{(1)}),\n",
    "$$\n",
    "where $z_1^{(1)}, z_2^{(1)}$ are activations:\n",
    "$$\n",
    "z_1 = w_{11}^{(1)}x_1 + w_{21}^{(1)}x_2 + b_1^{(1)},\\quad z_2 = w_{12}^{(1)}x_1 + w_{22}^{(1)}x_2 + b_2^{(1)},\n",
    "$$\n",
    "where $x_1, x_2$ are inputs, $w_{ij}^{(1)}, i,j\\in {1,2}^{(1)}$ are weights for the hidden layer, and $b_1^{(1)}, b_2^{(1)}$Â are biases for the hidden layer. Alternatively expressed with linear algebra:\n",
    "$$\n",
    "W^{(1)} =\n",
    "\\begin{bmatrix} \n",
    "w_{11}^{(1)} & w_{12}^{(1)} \\\\ \n",
    "w_{21}^{(1)} & w_{22}^{(1)}\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{b}^{(1)} = \n",
    "\\begin{bmatrix}\n",
    "b_1^{(1)}\\\\\n",
    "b_2^{(1)}\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{z}^{(1)} =\n",
    "\\begin{bmatrix}\n",
    "z_1^{(1)}\\\\\n",
    "z_2^{(1)}\n",
    "\\end{bmatrix}\n",
    "= W^{(1)^T} \\mathbf{x} + \\mathbf{b}^{(1)},\\quad\n",
    "\\mathbf{a}^{(1)}(\\mathbf{z}^{(1)}) = \n",
    "\\begin{bmatrix}\n",
    "a_1^{(1)}\\\\\n",
    "a_2^{(1)}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "We define the activation $z^{(2)}$ and output $y$ for the output layer:\n",
    "$$\n",
    "z^{(2)} =  a_1^{(1)} w_1^{(2)} + a_2^{(1)} w_2^{(2)} + b^{(2)},\\quad\n",
    "y = \\sigma(z^{(2)})\n",
    "$$\n",
    "where $w_1^{(2)}, w_2^{(2)}$ are weights for the output layer and $b^{(2)}$ is the bias for the output layer. Alternatively:\n",
    "$$\n",
    "\\mathbf{w}^{(2)} = \n",
    "\\begin{bmatrix}\n",
    "w_1^{(2)}\\\\\n",
    "w_2^{(2)}\n",
    "\\end{bmatrix},\\quad\n",
    "z^{(2)} = \\mathbf{w}^{(2)^T} \\mathbf{a}^{(1)} + b^{(2)}\n",
    "$$\n",
    "As such, the model produced by our FFNN is:\n",
    "$$\n",
    "y = \\sigma(\\mathbf{w}^{(2)}\\sigma(W^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)}) + b^{(2)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate our model against the target output $y$ with the loss function $C$ for which we use MSE. As such:\n",
    "$$\n",
    "C(y,g)\n",
    "$$\n",
    "alternatively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n",
      "[[ True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setting up a simple feed forward neural network\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def activation_function(activation):\n",
    "    return 1 / (1 + np.exp(-activation))\n",
    "\n",
    "\n",
    "features = np.array([[0, 0],\n",
    "                     [0, 1],\n",
    "                     [1, 0],\n",
    "                     [1, 1]])\n",
    "features = features.T\n",
    "print(features.shape)\n",
    "#_number denotes which layer \n",
    "weights_1 = np.ones((2, 2), dtype=float)\n",
    "biases_1 = np.random.randn(2, 1)\n",
    "weights_2 = np.ones((2, 1))\n",
    "bias_2 = np.random.normal()\n",
    "activation_1 = weights_1.T @ features + biases_1\n",
    "activation_2 = weights_2.T @ activation_function(activation_1) + bias_2\n",
    "target_output = activation_function(activation_2) \n",
    "print(target_output >0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a neural network is likely quite garbage if untrained. as such we update weights and biases to make a better model. To achieve this, we evaluate the neural network output $y$ in the loss function with the target value $t$. we collect all weights and biases in one vector\n",
    "$$\\boldsymbol{\\theta} = \n",
    "\\begin{bmatrix}\n",
    "w_11^{(1)}\\\\\n",
    "w_12^{(1)}\\\\\n",
    "w_21^{(1)}\\\\\n",
    "w_22^{(1)}\\\\\n",
    "w_1^{(2)}\\\\\n",
    "w_2^{(2)}\\\\\n",
    "b_1^{(1)}\\\\\n",
    "b_2^{(1)}\\\\\n",
    "b^{(2)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "And minimize C(t,y) w.r.t $\\boldsymbol{\\theta}$. We may do this with any of the gradient descent-like algorithms, and for all we require an expression of the gradient for $C$ w.r.t $\\boldsymbol{\\theta}$:\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial\\boldsymbol{\\theta}} = \\frac{\\partial C}{\\partial \\mathbf{y}}\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{a}_1}\\frac{\\partial \\mathbf{a_1}}{\\partial\\mathbf{z}}\\frac{\\partial \\mathbf{z}}{\\partial \\boldsymbol{\\theta}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.65044266 0.7250797  0.7250797  0.79442495]]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as np\n",
    "#import numpy as np\n",
    "from jax import grad\n",
    "from flatten_n_unflatten import flatten\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "parameters = (weights_1, weights_2, biases_1, bias_2)\n",
    "parameters_tuple, reshape_func = flatten(parameters)\n",
    "\n",
    "\n",
    "\n",
    "def simple_ffnn(parameters_tuple, features):\n",
    "  \"\"\"\n",
    "  Feed forwark neural network\n",
    "  \"\"\"\n",
    "  weights_1, weights_2, biases_1, bias_2 = reshape_func(parameters_tuple)\n",
    "  activation_1 = weights_1.T @ features + biases_1\n",
    "  activation_2 = weights_2.T @ activation_function(activation_1) + bias_2\n",
    "  target_output = activation_function(activation_2) \n",
    "  return target_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a = simple_ffnn(parameters_tuple, features)\n",
    "\n",
    "\n",
    "def loss(target, output):\n",
    "  \"\"\"\n",
    "  MSE\n",
    "  \"\"\"\n",
    "  return np.mean((target - output)**2)\n",
    "\n",
    "\n",
    "\n",
    "def meta_loss(parameters_tuple, target, features):\n",
    "  \"\"\"\n",
    "  Compute MSE for FFNN\n",
    "  \"\"\"\n",
    "  model_output = simple_ffnn(parameters_tuple, features)\n",
    "  mse = loss(target, model_output)\n",
    "  return mse\n",
    "\n",
    "\n",
    "def grad_meta_loss(parameters_tuple, target, features):\n",
    "  \"\"\"\n",
    "  compute gradient of meta loss w.r.t parameters\n",
    "  \"\"\"\n",
    "  return np.array(grad(meta_loss)(parameters_tuple, target, features))\n",
    "\n",
    "\n",
    "y_or = np.array([0,1,1,1])\n",
    "learning_rate = 0.01\n",
    "iteration = 0\n",
    "max_iter = 1000\n",
    "tolerance = 1e-5\n",
    "while True:\n",
    "    new_parameters_tuple = parameters_tuple - learning_rate*grad_meta_loss(parameters_tuple, y_or, features)\n",
    "    change = new_parameters_tuple - parameters_tuple\n",
    "    parameters_tuple = new_parameters_tuple\n",
    "    iteration += 1\n",
    "    if max_iter <= iteration or np.sqrt(np.sum(np.array(change))) < tolerance:\n",
    "      break\n",
    "output = simple_ffnn(parameters_tuple, features)\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
