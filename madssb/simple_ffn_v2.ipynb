{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the input\n",
    "$$\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "0 & 0\\\\\n",
    "1 & 0\\\\\n",
    "0 & 1\\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and outputs\n",
    "$$\n",
    "\\mathbf{y}_\\mathrm{OR} = \\begin{bmatrix}0 \\\\ 1 \\\\ 1 \\\\ 1\\end{bmatrix}, \\mathbf{y}_\\mathrm{XOR} = \\begin{bmatrix}0 \\\\ 1 \\\\ 1 \\\\ 0\\end{bmatrix}, \\mathbf{y}_\\mathrm{AND} = \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple  feed forward neural network (FFNN)\n",
    "We define the activation function with the sigmoid function $\\sigma(x)$ = \n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}.\n",
    "$$\n",
    "We define outputs $a_1^{(1)},a_2^{(1)}$ for the hidden layer with 2 nodes:\n",
    "$$\n",
    "a_1^{(1)} = \\sigma(z_1^{(1)}),\\quad a_2^{(1)} = \\sigma(z_2^{(1)}),\n",
    "$$\n",
    "where $z_1^{(1)}, z_2^{(1)}$ are activations:\n",
    "$$\n",
    "z_1 = w_{11}^{(1)}x_1 + w_{21}^{(1)}x_2 + b_1^{(1)},\\quad z_2 = w_{12}^{(1)}x_1 + w_{22}^{(1)}x_2 + b_2^{(1)},\n",
    "$$\n",
    "where $x_1, x_2$ are inputs, $w_{ij}^{(1)}, i,j\\in {1,2}^{(1)}$ are weights for the hidden layer, and $b_1^{(1)}, b_2^{(1)}$Â are biases for the hidden layer. Alternatively expressed with linear algebra:\n",
    "$$\n",
    "W^{(1)} =\n",
    "\\begin{bmatrix} \n",
    "w_{11}^{(1)} & w_{12}^{(1)} \\\\ \n",
    "w_{21}^{(1)} & w_{22}^{(1)}\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{b}^{(1)} = \n",
    "\\begin{bmatrix}\n",
    "b_1^{(1)}\\\\\n",
    "b_2^{(1)}\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{z}^{(1)} =\n",
    "\\begin{bmatrix}\n",
    "z_1^{(1)}\\\\\n",
    "z_2^{(1)}\n",
    "\\end{bmatrix}\n",
    "= W^{(1)^T} \\mathbf{x} + \\mathbf{b}^{(1)},\\quad\n",
    "\\mathbf{a}^{(1)}(\\mathbf{z}^{(1)}) = \n",
    "\\begin{bmatrix}\n",
    "a_1^{(1)}\\\\\n",
    "a_2^{(1)}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "We define the activation $z^{(2)}$ and output $y$ for the output layer:\n",
    "$$\n",
    "z^{(2)} =  a_1^{(1)} w_1^{(2)} + a_2^{(1)} w_2^{(2)} + b^{(2)},\\quad\n",
    "y = \\sigma(z^{(2)})\n",
    "$$\n",
    "where $w_1^{(2)}, w_2^{(2)}$ are weights for the output layer and $b^{(2)}$ is the bias for the output layer. Alternatively:\n",
    "$$\n",
    "\\mathbf{w}^{(2)} = \n",
    "\\begin{bmatrix}\n",
    "w_1^{(2)}\\\\\n",
    "w_2^{(2)}\n",
    "\\end{bmatrix},\\quad\n",
    "z^{(2)} = \\mathbf{w}^{(2)^T} \\mathbf{a}^{(1)} + b^{(2)}\n",
    "$$\n",
    "As such, the model produced by our FFNN is:\n",
    "$$\n",
    "y = \\sigma(\\mathbf{w}^{(2)}\\sigma(W^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)}) + b^{(2)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate our model against the target output $y$ with the loss function $C$ for which we use MSE. As such:\n",
    "$$\n",
    "C(y,g)\n",
    "$$\n",
    "alternatively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Simple Feed Forward Neural Network\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(activation):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid activation function.\n",
    "\n",
    "    The sigmoid function is a widely used activation function in neural networks,\n",
    "    often used to introduce non-linearity into the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    activation : numpy.ndarray\n",
    "        Input to the sigmoid function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Output after applying the sigmoid function element-wise.\n",
    "\n",
    "    Formula\n",
    "    -------\n",
    "    sigmoid(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> sigmoid(np.array([0.0, 2.0, -2.0]))\n",
    "    array([0.5       , 0.88079708, 0.11920292])\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-activation))\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss between true labels and predicted probabilities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : numpy.ndarray\n",
    "        True labels (ground truth), usually one-hot encoded.\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted probabilities, output from a softmax layer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cross-entropy loss.\n",
    "    \"\"\"\n",
    "    # Avoid division by zero and clip predicted values to a small positive value\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "    # Compute cross-entropy loss\n",
    "    loss = -np.sum(y_true * np.log(y_pred)) / len(y_true)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "features = np.array([[0, 0],\n",
    "                     [0, 1],\n",
    "                     [1, 0],\n",
    "                     [1, 1]])\n",
    "features = features.T\n",
    "#_number denotes which layer \n",
    "\"\"\"Hidden layer\n",
    "\"\"\"\n",
    "weights_1 = np.ones((2, 2), dtype=float)\n",
    "biases_1 = np.random.randn(2, 1)\n",
    "activation_1 = weights_1.T @ features + biases_1\n",
    "hidden_layer_output = sigmoid(activation_1)\n",
    "\"\"\"Output layer\n",
    "\"\"\"\n",
    "weights_2 = np.ones((2, 1))\n",
    "bias_2 = np.random.normal()\n",
    "activation_2 = weights_2.T @ + bias_2\n",
    "output = cross(activation_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a neural network is likely quite garbage if untrained. as such we update weights and biases to make a better model. To achieve this, we evaluate the neural network output $y$ in the loss function with the target value $t$. we collect all weights and biases in one vector\n",
    "$$\\boldsymbol{\\theta} = \n",
    "\\begin{bmatrix}\n",
    "w_11^{(1)}\\\\\n",
    "w_12^{(1)}\\\\\n",
    "w_21^{(1)}\\\\\n",
    "w_22^{(1)}\\\\\n",
    "w_1^{(2)}\\\\\n",
    "w_2^{(2)}\\\\\n",
    "b_1^{(1)}\\\\\n",
    "b_2^{(1)}\\\\\n",
    "b^{(2)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "And minimize C(t,y) w.r.t $\\boldsymbol{\\theta}$. We may do this with any of the gradient descent-like algorithms, and for all we require an expression of the gradient for $C$ w.r.t $\\boldsymbol{\\theta}$:\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial\\boldsymbol{\\theta}} = \\frac{\\partial C}{\\partial \\mathbf{y}}\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{a}_1}\\frac{\\partial \\mathbf{a_1}}{\\partial\\mathbf{z}}\\frac{\\partial \\mathbf{z}}{\\partial \\boldsymbol{\\theta}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
